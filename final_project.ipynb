{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b11272-fea2-42f8-abf7-39bbf76a875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, re, csv, random, networkx as nx, argparse\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from Vocabulary import Vocabulary\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "import nltk, spacy\n",
    "from spellchecker import SpellChecker\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37fe60-5e55-42a6-a120-3e35ee4c3309",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NLP_MODEL    = \"en_core_web_sm\"\n",
    "SBERT_MODEL  = \"all-mpnet-base-v2\"\n",
    "SENT_OUT     = 128\n",
    "BATCH_SIZE   = 16\n",
    "MAX_TOKENS   = 300\n",
    "EMB_DIM      = 100\n",
    "HID_DIM      = 300\n",
    "OUT_DIM      = 64\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "lp    = spacy.load(NLP_MODEL, disable=[\"ner\"])\n",
    "spell = SpellChecker()\n",
    "sbert = SentenceTransformer(SBERT_MODEL, device=str(DEVICE))\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b07f65-d49c-402f-ac48-912522d30d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_spliter(path_to_dataset, train_path, valid_path, test_path):\n",
    "    with open(path_to_dataset, newline='') as dataset:\n",
    "\n",
    "        reader = csv.DictReader(dataset)\n",
    "\n",
    "        with open(train_path, 'w', newline='') as train, open(valid_path, 'w', newline='') as valid, open(test_path, 'w', newline='') as test:\n",
    "            train_writer = csv.DictWriter(train, fieldnames=reader.fieldnames)\n",
    "            valid_writer = csv.DictWriter(valid, fieldnames=reader.fieldnames)\n",
    "            test_writer = csv.DictWriter(test, fieldnames=reader.fieldnames)\n",
    "\n",
    "            train_writer.writeheader()\n",
    "            valid_writer.writeheader()\n",
    "            test_writer.writeheader()\n",
    "\n",
    "            for num, row in enumerate(reader):\n",
    "                if num < 24728 * 0.6:\n",
    "                    train_writer.writerow(row)\n",
    "                elif num < 24728 * 0.8:\n",
    "                    valid_writer.writerow(row)\n",
    "                else:\n",
    "                    test_writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4929894-23e0-4e51-99b4-06385451e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def misSpell_counter(text):\n",
    "    spell = SpellChecker()\n",
    "    words = text.split()\n",
    "    misspelled = spell.unknown(words)\n",
    "\n",
    "    corrected_words = [\n",
    "        spell.correction(w) or w if w in misspelled else w for w in words]\n",
    "    if len(corrected_words) > 0:\n",
    "        return len(misspelled), \" \".join(corrected_words)\n",
    "    else:\n",
    "        return len(misspelled), None    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2e185-db56-4f24-8b93-29510c55c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_essay(text):\n",
    "    error_count, cleaned_text = misSpell_counter(text)\n",
    "    tokens = nltk.word_tokenize(cleaned_text)\n",
    "    shallow_features = {}\n",
    "    # ('spelling_errors', 'num_words', 'num_sentences', 'num_sentence_length', 'sentence_variance', 'num_characters', 'num_nouns', 'num_verbs', 'num_adverbs', 'num_adverbs', 'num_conjunctions',\n",
    "                        # 'num_adjectives', 'num_characters', 'mean_wordLength', 'distinct_words', num_punctuations)\n",
    "    \n",
    "    shallow_features['spelling_errors'] = error_count\n",
    "    words = text.split()\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    shallow_features['num_words'] = len(words)\n",
    "    shallow_features['num_punctuations'] = len(tokens) - len(words)\n",
    "    shallow_features['num_sentences'] = len(sents)\n",
    "    shallow_features['num_sentence_length'] = shallow_features['num_words'] / shallow_features['num_sentences']\n",
    "    lengths = [len(nltk.word_tokenize(sent)) for sent in sents]\n",
    "    shallow_features['sentence_variance'] = statistics.variance(lengths) if len(lengths) > 1 else 0.0\n",
    "    \n",
    "    shallow_features['num_characters'] = 0\n",
    "    shallow_features['num_nouns'] = 0\n",
    "    shallow_features['num_verbs'] = 0\n",
    "    shallow_features['num_adverbs'] = 0\n",
    "    shallow_features['num_conjunctions'] = 0\n",
    "    shallow_features['num_adjectives'] = 0\n",
    "\n",
    "    distinct_words = set()\n",
    "    for word in words:\n",
    "        shallow_features['num_characters'] += len(word)\n",
    "    \n",
    "    shallow_features['mean_wordLength'] = shallow_features['num_characters'] / shallow_features['num_words']\n",
    "    tagged = nltk.pos_tag(words, tagset='universal')\n",
    "    for t in tagged:\n",
    "        if t[1] == 'NOUN':\n",
    "            shallow_features['num_nouns'] += 1\n",
    "        elif t[1] == 'VERB':\n",
    "            shallow_features['num_verbs'] += 1\n",
    "        elif t[1] == 'ADV':\n",
    "            shallow_features['num_adverbs'] += 1\n",
    "        elif t[1] == 'CONJ':\n",
    "            shallow_features['num_conjunctions'] += 1\n",
    "        elif t[1] == 'ADJ':\n",
    "            shallow_features['num_adjectives'] += 1\n",
    "        distinct_words.add(t[0])\n",
    "    shallow_features[\"distinct_words\"] = len(distinct_words)\n",
    "\n",
    "    return cleaned_text, shallow_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8ec98-f4b7-48fa-8739-9cd1bb24a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocEncoder(nn.Module):\n",
    "    def __init__(self, vocab):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab, EMB_DIM, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(EMB_DIM, HID_DIM, batch_first=True)\n",
    "        self.proj = nn.Sequential(nn.Dropout(0.4), nn.Linear(HID_DIM, OUT_DIM), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, ids, lens):\n",
    "        x = self.emb(ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h, _) = self.lstm(packed)\n",
    "        return self.proj(h.squeeze(0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3ca59-aacf-465f-b1a8-d4b50ed2d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "class _SentenceEmbeddingLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 freeze_bert=True):\n",
    "        super().__init__()\n",
    "        self.bert = SentenceTransformer(pretrained_model)\n",
    "        if freeze_bert:\n",
    "            for p in self.bert.parameters():\n",
    "                p.requires_grad = False\n",
    "        self.embed_dim = self.bert.get_sentence_embedding_dimension()\n",
    "\n",
    "    def forward(self, raw_sentences: list[str]) -> torch.Tensor:\n",
    "\n",
    "        non_empty, mapping = [], []\n",
    "        for s in raw_sentences:\n",
    "            if s.strip():\n",
    "                mapping.append(len(non_empty))\n",
    "                non_empty.append(s)\n",
    "            else:\n",
    "                mapping.append(-1)\n",
    "\n",
    "        if non_empty:\n",
    "            embs = self.bert.encode(non_empty, convert_to_tensor=True)\n",
    "        else:\n",
    "            device = next(self.parameters()).device\n",
    "            embs = torch.zeros((1, self.embed_dim), device=device)\n",
    "\n",
    "        out = torch.zeros((len(raw_sentences), self.embed_dim),\n",
    "                          device=embs.device)\n",
    "        for i, j in enumerate(mapping):\n",
    "            if j >= 0:\n",
    "                out[i] = embs[j]\n",
    "        return out\n",
    "\n",
    "\n",
    "class _AttentionPooling(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.att = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_out, essay_lens):\n",
    "\n",
    "        scores = self.att(lstm_out).squeeze(-1)\n",
    "\n",
    "        max_sent = scores.size(1)\n",
    "        mask = (torch.arange(max_sent, device=scores.device)\n",
    "                .unsqueeze(0) < essay_lens.unsqueeze(1))\n",
    "        scores = scores.masked_fill(~mask, -1e9)\n",
    "\n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        pooled  = torch.sum(lstm_out * weights.unsqueeze(-1), dim=1)\n",
    "        return pooled, weights\n",
    "\n",
    "\n",
    "class SentenceEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 freeze_bert=True,\n",
    "                 lstm_hidden_dim=256,\n",
    "                 dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sentence_embedding = _SentenceEmbeddingLayer(pretrained_model,\n",
    "                                                          freeze_bert)\n",
    "        emb_dim = self.sentence_embedding.embed_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim,\n",
    "                            lstm_hidden_dim,\n",
    "                            num_layers=1,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=True,\n",
    "                            dropout=0.0)\n",
    "\n",
    "        self.attention_pool = _AttentionPooling(lstm_hidden_dim * 2)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_hidden_dim * 2, lstm_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, flat_sentences, essay_lengths, batch_size, max_sentences):\n",
    "        \n",
    "        sent_emb = self.sentence_embedding(flat_sentences)\n",
    "        E = sent_emb.size(-1)\n",
    "    \n",
    "        padded = sent_emb.new_zeros(batch_size, max_sentences, E)\n",
    "    \n",
    "        idx = 0\n",
    "        for b, n_sents in enumerate(essay_lengths.tolist()):\n",
    "            if n_sents:\n",
    "                padded[b, :n_sents] = sent_emb[idx: idx + n_sents]\n",
    "            idx += n_sents\n",
    "    \n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            padded, essay_lengths.cpu(),\n",
    "            batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        lstm_out, _ = self.lstm(packed)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            lstm_out, batch_first=True, total_length=max_sentences\n",
    "        )\n",
    "    \n",
    "        pooled, att_w = self.attention_pool(unpacked, essay_lengths)\n",
    "        scores = self.classifier(pooled)\n",
    "        return scores.squeeze(-1), att_w\n",
    "\n",
    "\n",
    "def load_sentence_encoder(ckpt_path: str | Path,\n",
    "                          device: str = \"cpu\",\n",
    "                          freeze: bool = True) -> SentenceEncoder:\n",
    "    enc = SentenceEncoder()\n",
    "    raw = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "    new_state = {}\n",
    "    for k, v in raw.items():\n",
    "\n",
    "        if k.startswith(\"attention_pool.attention.\"):\n",
    "\n",
    "            k = k.replace(\"attention_pool.attention.\", \"attention_pool.att.\")\n",
    "\n",
    "        if re.fullmatch(r\"lstm\\.(weight|bias)_(ih|hh)_l1(_reverse)?\", k):\n",
    "\n",
    "            continue\n",
    "        new_state[k] = v\n",
    "\n",
    "    missing, unexpected = enc.load_state_dict(new_state, strict=False)\n",
    "    if missing:\n",
    "        print(\"[SentenceEncoder]   • ignored missing keys:\", missing)\n",
    "    if unexpected:\n",
    "        print(\"[SentenceEncoder]   • ignored extra keys  :\", unexpected)\n",
    "\n",
    "    if freeze:\n",
    "        enc.eval()\n",
    "        for p in enc.parameters():\n",
    "            p.requires_grad_(False)\n",
    "    return enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0efe81-80fb-4fc2-b150-9a5bd2e23ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_df = pd.read_excel(args.prompts_xlsx)\n",
    "\n",
    "if not {\"essay_set\", \"essay_description\"}.issubset(prompts_df.columns):\n",
    "    raise ValueError(\"Excel must have columns 'essay_set' and 'essay_description'\")\n",
    "\n",
    "PROMPT_TEXT = {int(r.essay_set): str(r.essay_description)\n",
    "               for r in prompts_df.itertuples()}\n",
    "\n",
    "print(\"Loaded prompt texts:\", PROMPT_TEXT.keys())\n",
    "\n",
    "PROMPT_EMB = {pid: sbert.encode(txt, convert_to_tensor=True)\n",
    "              for pid, txt in PROMPT_TEXT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13d6f9-1cd1-4a04-bb56-171bfa3256f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptSimilarity(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    @torch.no_grad()\n",
    "    def forward(self, essays: List[str], prompt_ids: torch.Tensor) -> torch.Tensor:\n",
    "        essay_emb = sbert.encode(essays, convert_to_tensor=True)\n",
    "        pe = torch.stack([PROMPT_EMB[int(pid.item())] for pid in prompt_ids]\n",
    "                         ).to(essay_emb)\n",
    "        cos = F.cosine_similarity(essay_emb, pe)\n",
    "        return cos.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fee3e89-ce0a-401d-abae-7d66e374d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AESModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, sent_ckpt: Path):\n",
    "        super().__init__()\n",
    "        self.doc   = DocEncoder(vocab_size)\n",
    "        self.sent  = load_sentence_encoder(sent_ckpt)\n",
    "        self.promp = PromptSimilarity()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_FH   = torch.zeros(1, 14)\n",
    "            dummy_ids  = torch.zeros(1, MAX_TOKENS,  dtype=torch.long)\n",
    "            dummy_lens = torch.tensor([1])\n",
    "            dummy_FS   = torch.zeros(1,1)\n",
    "            dummy_FT   = torch.zeros(1,1)\n",
    "            dummy_FD   = torch.zeros(1, OUT_DIM)\n",
    "            dummy = torch.cat([dummy_FH, dummy_FD, dummy_FS, dummy_FT], 1)\n",
    "            F_total = dummy.size(1)\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(F_total, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, FH, ids, lens, prompt_ids, sent_lists=[], essays=[]):\n",
    "        FD = self.doc(ids, lens)\n",
    "        flat_sents   = sum(sent_lists, [])\n",
    "        essay_lens   = torch.tensor([len(s) for s in sent_lists],\n",
    "                                    device=ids.device)\n",
    "        if len(essays) > 0:\n",
    "            B, M = len(sent_lists), max(essay_lens).item()\n",
    "        else:\n",
    "            B, M = 0,0\n",
    "\n",
    "        FS, _ = self.sent(flat_sents, essay_lens, B, M)\n",
    "        FS = FS.unsqueeze(1)\n",
    "\n",
    "        FT = self.promp(essays, prompt_ids)\n",
    "\n",
    "        fused = torch.cat([FH, FD, FS, FT], dim=1)\n",
    "        return self.head(fused).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703c017-87c3-41e3-af79-837e09311c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "CKPT_FILE = \"checkpoint.pt\"\n",
    "\n",
    "def save_ckpt(epoch, best_qwk, model, optimizer):\n",
    "    torch.save({\n",
    "        \"epoch\"     : epoch,\n",
    "        \"best_qwk\"  : best_qwk,\n",
    "        \"model\"     : model.state_dict(),\n",
    "        \"optim\"     : optimizer.state_dict()\n",
    "    }, CKPT_FILE)\n",
    "    print(f\"✓ checkpoint saved: {CKPT_FILE} (epoch {epoch})\")\n",
    "\n",
    "def load_ckpt(model, optimizer):\n",
    "    ckpt = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optim\"])\n",
    "    print(f\"↻ resumed from {CKPT_FILE} (epoch {ckpt['epoch']})\")\n",
    "    return ckpt[\"epoch\"], ckpt[\"best_qwk\"]\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                train_loader: DataLoader,\n",
    "                val_loader  : DataLoader | None = None,\n",
    "                epochs: int = 10):\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    opt       = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "    loss_fn   = nn.MSELoss()\n",
    "    best_qwk  = -1.0\n",
    "    start_ep  = 1\n",
    "\n",
    "    if os.path.exists(CKPT_FILE):\n",
    "        start_ep, best_qwk = load_ckpt(model, opt)\n",
    "        start_ep += 1\n",
    "\n",
    "    max_norm = 1.0\n",
    "\n",
    "    for ep in range(start_ep, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader, start=1):\n",
    "            FH, ids, ln, lab, mx, sent_lists, texts, pids = batch\n",
    "            FH, ids, ln, lab = FH.to(DEVICE), ids.to(DEVICE), ln.to(DEVICE), lab.to(DEVICE)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            pred  = model(FH, ids, ln, sent_lists, texts, pids)\n",
    "            loss  = loss_fn(pred, lab)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            opt.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            msg = f\"[{ep:02d}/{epochs}] train-MSE {epoch_loss:.4f} step: {step}\"\n",
    "            print(msg)        \n",
    "\n",
    "        if val_loader is not None:\n",
    "            qwk_val = evaluate(model, val_loader)\n",
    "            msg += f\" | val-QWK {qwk_val:.4f}\"\n",
    "            if qwk_val > best_qwk:\n",
    "                best_qwk = qwk_val\n",
    "                torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(msg)\n",
    "\n",
    "        save_ckpt(ep, best_qwk, model, opt)\n",
    "\n",
    "    if val_loader is None:\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0636a7-5c94-400b-b2ff-53b8a5ff29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vocab: Dict[str,int], has_labels: bool):\n",
    "        self.df         = df.reset_index(drop=True)\n",
    "        self.vocab      = vocab\n",
    "        self.has_labels = has_labels\n",
    "\n",
    "    def _encode(self, txt: str):\n",
    "        ids = self.vocab.text2idx(txt)[:MAX_TOKENS]\n",
    "        ids += [0] * (MAX_TOKENS - len(ids))\n",
    "        return torch.tensor(ids, dtype=torch.long), len(ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r        = self.df.iloc[idx]\n",
    "        txt      = r[\"essay\"]\n",
    "        pid      = int(r[\"prompt_id\"])\n",
    "        ids, ln  = self._encode(txt)\n",
    "\n",
    "        _, feat_dict = preprocess_essay(txt)\n",
    "        FH = torch.tensor(list(feat_dict.values()), dtype=torch.float32)\n",
    "\n",
    "        sent_list = nltk.sent_tokenize(txt)\n",
    "\n",
    "        if self.has_labels:\n",
    "            score_norm = torch.tensor(r[\"score_norm\"], dtype=torch.float32)\n",
    "        else:\n",
    "            score_norm = torch.tensor(0.0, dtype=torch.float32)\n",
    "\n",
    "        max_score = torch.tensor(r[\"max_score\"], dtype=torch.float32)\n",
    "        pid_tensor= torch.tensor(pid, dtype=torch.long)\n",
    "\n",
    "        return (\n",
    "            FH,\n",
    "            ids,\n",
    "            torch.tensor(ln, dtype=torch.long),\n",
    "            score_norm,\n",
    "            max_score,\n",
    "            sent_list,\n",
    "            txt,\n",
    "            pid_tensor\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daf6a77-f5e4-41a3-ae03-ca7e77570189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "def essay_collate(batch):\n",
    "    with_labels = len(batch[0]) == 8\n",
    "\n",
    "    if with_labels:\n",
    "        FH, ids, ln, score_norm, max_score, sent_list, texts, pid = zip(*batch)\n",
    "        score_norm = torch.stack(score_norm)\n",
    "    else:\n",
    "        FH, ids, ln,        max_score, sent_list, texts, pid = zip(*batch)\n",
    "        score_norm = None\n",
    "\n",
    "    FH        = torch.stack(FH)\n",
    "    ids       = torch.stack(ids)\n",
    "    ln        = torch.stack(ln)\n",
    "    max_score = torch.stack(max_score)\n",
    "    pid       = torch.stack(pid)\n",
    "\n",
    "    if with_labels:\n",
    "        return FH, ids, ln, score_norm, max_score, list(sent_list), list(texts), pid\n",
    "    else:\n",
    "        return FH, ids, ln, score_norm, max_score, list(sent_list), list(texts), pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44059f9f-963e-40a3-befc-c34cbae8eefd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_path = \"/nfs/stak/users/mettas/ondemand/AI539_HPC/Datasets/asap-aes/training_set_rel3.tsv\"\n",
    "val_path = \"/nfs/stak/users/mettas/ondemand/AI539_HPC/Datasets/asap-aes/valid_set.tsv\"\n",
    "prompt_path = \"/nfs/stak/users/mettas/ondemand/AI539_HPC/Datasets/asap-aes/Essay_Set_Descriptions/Essay_Set_Descriptions/essay_set_descriptions.xlsx\"\n",
    "sent_ckpt_path = \"/nfs/stak/users/mettas/ondemand/AI539_HPC/Final_Project/best_model_SBERT0607_1712_NbrnTP_SBERT_LSTM.pth\"\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train\",       default=train_path)\n",
    "parser.add_argument(\"--valid\",       default=val_path)\n",
    "parser.add_argument(\"--sent-ckpt\",   default=sent_ckpt_path)\n",
    "parser.add_argument(\"--prompts-xlsx\",default=prompt_path)\n",
    "args = parser.parse_args([])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da97263a-4641-477a-a66f-17f26a1ef774",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    col_map={\"essay_set\":\"prompt_id\",\"domain1_score\":\"score\"}\n",
    "    tr_df = pd.read_csv(args.train, sep=\"\\t\", quoting=csv.QUOTE_NONE, engine=\"python\",\n",
    "                        encoding=\"ISO-8859-1\").rename(columns=col_map)[[\"prompt_id\",\"essay\",\"score\"]]\n",
    "    va_df = pd.read_csv(args.valid, sep=\"\\t\", quoting=csv.QUOTE_NONE, engine=\"python\",\n",
    "                        encoding=\"ISO-8859-1\")\n",
    "\n",
    "    max_scores = pd.concat([tr_df]).groupby(\"prompt_id\")[\"score\"].max().to_dict()\n",
    "    tr_df[\"max_score\"]=tr_df[\"prompt_id\"].map(max_scores)\n",
    "    tr_df[\"score_norm\"]=tr_df[\"score\"]/tr_df[\"max_score\"]\n",
    "\n",
    "    vocab = Vocabulary(tr_df[\"essay\"].tolist(), min_freq=1)\n",
    "    print(\"Vocab size:\", vocab.size)\n",
    "    train_ds = EssayDataset(tr_df, vocab, True)\n",
    "    val_ds   = EssayDataset(va_df, vocab, False)\n",
    "    train_ld = DataLoader(train_ds,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      shuffle=True,\n",
    "                      num_workers=2,\n",
    "                      pin_memory=True,\n",
    "                      collate_fn=essay_collate)\n",
    "    # if val_loader is not None:\n",
    "    #     val_ds = EssayDataset(val_df, vocab, with_labels=True)\n",
    "    #     val_loader = DataLoader(\n",
    "    #         val_ds,\n",
    "    #         batch_size=32,\n",
    "    #         shuffle=False,\n",
    "    #         num_workers=2,\n",
    "    #         pin_memory=True,\n",
    "    #         collate_fn=essay_collate\n",
    "    #     )\n",
    "\n",
    "    model = AESModel(vocab.size, Path(args.sent_ckpt))\n",
    "    train_model(model,train_ld,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3758f44f-48fe-4499-91e0-0a69133045f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, torch, csv, json, os\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def fisher_z(k):\n",
    "    k = np.clip(k, -0.999, 0.999)\n",
    "    return .5*np.log((1+k)/(1-k))\n",
    "\n",
    "def fisher_mean(kappa_dict: dict[int,float]) -> float:\n",
    "    if not kappa_dict:\n",
    "        return np.nan\n",
    "    z  = [fisher_z(v) for v in kappa_dict.values()]\n",
    "    return float(np.tanh(np.mean(z)))\n",
    "\n",
    "def overall_kappa(df: pd.DataFrame) -> float:\n",
    "    per_prompt = {p: cohen_kappa_score(g.score, g.predicted_score,\n",
    "                                       weights=\"quadratic\")\n",
    "                  for p, g in df.groupby(\"prompt_id\")}\n",
    "    return fisher_mean(per_prompt)\n",
    "\n",
    "def run_and_score_v2(model: torch.nn.Module,\n",
    "                     csv_path          : str,\n",
    "                     vocab             : dict[str,int],\n",
    "                     train_max_scores  : dict[int,int]|None = None,\n",
    "                     batch_size        : int = 32,\n",
    "                     with_labels       : bool = True,\n",
    "                     out_csv           : str = \"submission.csv\"):\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    col_rename = {\"full_text\":\"essay\",\n",
    "                  \"assignment\":\"prompt_tag\",\n",
    "                  \"score\":\"score\"}\n",
    "    df = df.rename(columns=col_rename)\n",
    "\n",
    "    if \"essay_id\" not in df.columns or \"essay\" not in df.columns \\\n",
    "       or \"prompt_tag\" not in df.columns:\n",
    "        raise ValueError(\"CSV must contain essay_id / full_text / assignment\")\n",
    "\n",
    "    prompt_lookup  = {tag:i for i, tag in\n",
    "                      enumerate(sorted(df[\"prompt_tag\"].unique()), start=1)}\n",
    "    df[\"prompt_id\"] = train_max_scores\n",
    "\n",
    "    if with_labels and train_max_scores is None:\n",
    "        train_max_scores = df.groupby(\"prompt_id\")[\"score\"].max().to_dict()\n",
    "    if train_max_scores is None:\n",
    "        raise ValueError(\"train_max_scores must be supplied for test data\")\n",
    "\n",
    "    df[\"max_score\"] = train_max_scores\n",
    "\n",
    "    if with_labels:\n",
    "        df[\"score_norm\"] = df[\"score\"] / df[\"max_score\"]\n",
    "\n",
    "    ds = EssayDataset(df, vocab, has_labels=with_labels)\n",
    "    ld = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=2, pin_memory=True, collate_fn=essay_collate)\n",
    "\n",
    "    DEVICE = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in ld:\n",
    "            FH, ids, ln, _, _, sent_lists, texts, pids = batch\n",
    "            FH, ids, ln, pids = (FH.to(DEVICE),\n",
    "                                 ids.to(DEVICE),\n",
    "                                 ln.to(DEVICE),\n",
    "                                 pids.to(DEVICE))\n",
    "    \n",
    "            out = model(FH, ids, ln, pids, sent_lists, texts)\n",
    "            preds.append(out.cpu())\n",
    "    \n",
    "    df[\"predicted_score\"] = np.round(\n",
    "        torch.cat(preds).numpy() * df[\"max_score\"].values\n",
    "    ).astype(int)\n",
    "\n",
    "    if with_labels:\n",
    "        kappa_overall = overall_kappa(df)\n",
    "        print(f\"Overall QWK (Fisher mean): {kappa_overall:0.4f}\")\n",
    "\n",
    "        per_prompt = {p: cohen_kappa_score(g.score, g.predicted_score,\n",
    "                                           weights=\"quadratic\")\n",
    "                      for p, g in df.groupby(\"prompt_id\")}\n",
    "        print(\"Per-prompt kappas:\", per_prompt)\n",
    "\n",
    "    df[[\"essay_id\", \"predicted_score\"]].to_csv(out_csv, index=False)\n",
    "    print(\"Saved predictions ➜\", out_csv)\n",
    "\n",
    "    return df if with_labels else None\n",
    "\n",
    "model = AESModel(vocab.size, Path(args.sent_ckpt))\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "validation_path = \"/nfs/stak/users/mettas/ondemand/AI539_HPC/Datasets/ASAP/Valid.csv\"\n",
    "train_max = 5\n",
    "run_and_score_v2(\n",
    "        model,\n",
    "        csv_path      = validation_path,\n",
    "        vocab         = vocab,\n",
    "        train_max_scores = train_max,\n",
    "        with_labels   = True,\n",
    "        out_csv       = \"valid_preds.csv\")\n",
    "\n",
    "run_and_score(model,\n",
    "                tsv_path=\"/nfs/stak/users/mettas/ondemand/AI539_HPC/Datasets/asap-aes/test_set.tsv\",\n",
    "                with_labels=False,\n",
    "                vocab=vocab,\n",
    "                stub_csv=None,\n",
    "                out_csv=\"test_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c4322a-67e6-4684-95db-39195bfc63e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
